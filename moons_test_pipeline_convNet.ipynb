{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1SQVGd9CMseUtXGFHXp3RR7OsRABnvRo3",
      "authorship_tag": "ABX9TyO2Y1hxQm0IWWokwBmwJrug"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a1DxnjrMe5Q"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan 16\n",
        "\n",
        "@author: mginolfi\n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import callbacks\n",
        "import tensorflow as tf\n",
        "import scipy.ndimage\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.interpolate import interp1d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrive Data and save locally\n",
        "Run the function or just open the link"
      ],
      "metadata": {
        "id": "kCcyc5n8hE7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_file(url, local_filename):\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(requests.get(url).content)\n",
        "\n",
        "url = input(\"https://drive.google.com/file/d/1QNStvFf2OIBBlcKw1hMhtgDHt5hM2u3s/view?usp=drive_link\")\n",
        "filename = input(\"final_combined_dataset.pickle\")\n",
        "download_file(url, filename)\n"
      ],
      "metadata": {
        "id": "RLWzNe8uwUsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset preparation (run once)"
      ],
      "metadata": {
        "id": "cYk843Cbhgbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" load the combined dataset pickle file and split in train - validation - test. save local copies \"\"\"\n",
        "\n",
        "# Read the combined dataset\n",
        "reconstructed_df = pd.read_pickle('final_combined_dataset.pickle')\n",
        "\n",
        "# show keys\n",
        "reconstructed_df.columns\n",
        "\n",
        "# First Split: Train (including validation) and Test\n",
        "df_train_val, df_test = train_test_split(reconstructed_df, test_size=0.15, random_state=42) # 15% test\n",
        "\n",
        "# Second Split: Train and Validation from X_temp and Y_temp\n",
        "# Note: 15% of the remaining 85% is 0.1765 (approximately 17.65%)\n",
        "df_train, df_val = train_test_split(df_train_val, test_size=0.1765, random_state=42) # About 15% of total\n",
        "\n",
        "# Saving the Split DataFrames\n",
        "df_train.to_pickle('train_dataset.pickle')\n",
        "df_val.to_pickle('validation_dataset.pickle')\n",
        "df_test.to_pickle('test_dataset.pickle')"
      ],
      "metadata": {
        "id": "j-gIvnQlwVvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading & pre-process data\n"
      ],
      "metadata": {
        "id": "k4_xLuKYhnwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rebin_spectrum_with_interpolation(spectra, rebin_factor):\n",
        "    \"\"\"\n",
        "    Rebins the spectra by a specified factor using interpolation.\n",
        "\n",
        "    Parameters:\n",
        "    spectra (numpy.ndarray): 2D array of spectra with shape (n_spectra, n_channels)\n",
        "    rebin_factor (int): Factor by which to rebin the spectra.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Re-binned spectra.\n",
        "    \"\"\"\n",
        "\n",
        "    n_spectra, n_channels = spectra.shape\n",
        "    new_n_channels = n_channels // rebin_factor\n",
        "\n",
        "    # Create an interpolation function for each spectrum\n",
        "    rebinned_spectra = np.zeros((n_spectra, new_n_channels))\n",
        "    for i in range(n_spectra):\n",
        "        x = np.linspace(0, n_channels, n_channels)\n",
        "        y = spectra[i, :]\n",
        "        f = interp1d(x, y, kind='linear')\n",
        "\n",
        "        # New x values\n",
        "        new_x = np.linspace(0, n_channels, new_n_channels)\n",
        "        rebinned_spectra[i, :] = f(new_x)\n",
        "\n",
        "    return rebinned_spectra"
      ],
      "metadata": {
        "id": "6TVCKpqHwc9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" load training set, make X & Y dataset and normalise\"\"\"\n",
        "\n",
        "df_train = pd.read_pickle('train_dataset.pickle')\n",
        "\n",
        "# mic (filippo's test)\n",
        "# Filter the dataframe\n",
        "df_train = df_train[df_train['MAG_vista-H'] < 24]\n",
        "\n",
        "# Extract features for training set\n",
        "all_spectra_train = np.stack(df_train['combined_spectrum'].values)\n",
        "all_skyFlux_train = np.stack(df_train['combined_skyMask'].values)\n",
        "\n",
        "# Normalization of training spectra\n",
        "max_value_train = all_spectra_train.max()\n",
        "all_spectra_train_normalized = all_spectra_train / max_value_train\n",
        "\n",
        "# rebin spectra with interpolation\n",
        "all_spectra_train_normalized = rebin_spectrum_with_interpolation(all_spectra_train_normalized, 1)\n",
        "all_skyFlux_train = rebin_spectrum_with_interpolation(all_skyFlux_train, 1)\n",
        "\n",
        "# make X train\n",
        "X_train = np.stack((all_spectra_train_normalized, all_skyFlux_train), axis=-1)\n",
        "\n",
        "# Extract labels for training set\n",
        "all_redshift_train = df_train['z'].values\n",
        "all_stellar_masses_train = df_train['log_m'].values\n",
        "all_sfr_train = np.log10(df_train['sfr'].values)\n",
        "\n",
        "# define labels: make Y\n",
        "Y_train = np.column_stack((all_redshift_train, all_stellar_masses_train, all_sfr_train))\n",
        "\n",
        "# Calculate mean and standard deviation for each label type in the training set\n",
        "Y_train_mean = Y_train.mean(axis=0)\n",
        "Y_train_std = Y_train.std(axis=0)\n",
        "\n",
        "# Normalize training labels\n",
        "Y_train_normalized = (Y_train - Y_train_mean) / Y_train_std\n",
        "\n",
        "del df_train\n",
        "del all_spectra_train\n",
        "del all_skyFlux_train\n"
      ],
      "metadata": {
        "id": "UW1pFn3NwhnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" load validation set, make X & Y dataset and normalise\"\"\"\n",
        "\n",
        "df_val = pd.read_pickle('validation_dataset.pickle')\n",
        "\n",
        "# mic (filippo's test)\n",
        "# Filter the dataframe\n",
        "df_val = df_val[df_val['MAG_vista-H'] < 24]\n",
        "\n",
        "# Extract features for validation set\n",
        "all_spectra_val = np.stack(df_val['combined_spectrum'].values)\n",
        "all_skyFlux_val = np.stack(df_val['combined_skyMask'].values)\n",
        "\n",
        "# Normalization of validation spectra\n",
        "all_spectra_val_normalized = all_spectra_val / max_value_train\n",
        "\n",
        "# rebin spectra with interpolation\n",
        "all_spectra_val_normalized = rebin_spectrum_with_interpolation(all_spectra_val_normalized, 1)\n",
        "all_skyFlux_val = rebin_spectrum_with_interpolation(all_skyFlux_val, 1)\n",
        "\n",
        "# make X val\n",
        "X_val = np.stack((all_spectra_val_normalized, all_skyFlux_val), axis=-1)\n",
        "\n",
        "# Extract labels for validation set\n",
        "all_redshift_val = df_val['z'].values\n",
        "all_stellar_masses_val = df_val['log_m'].values\n",
        "all_sfr_val = np.log10(df_val['sfr'].values)\n",
        "\n",
        "# define labels\n",
        "Y_val = np.column_stack((all_redshift_val, all_stellar_masses_val, all_sfr_val))\n",
        "\n",
        "# Normalize validation labels\n",
        "Y_val_normalized = (Y_val - Y_train_mean) / Y_train_std\n",
        "\n",
        "del df_val\n",
        "del all_spectra_val\n",
        "del all_skyFlux_val"
      ],
      "metadata": {
        "id": "coL44CwXwjWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" load test set, make X & Y dataset and normalise\"\"\"\n",
        "\n",
        "df_test = pd.read_pickle('test_dataset.pickle')\n",
        "\n",
        "df_test.columns\n",
        "\n",
        "# mic (filippo's test)\n",
        "# Filter the dataframe\n",
        "df_test = df_test[df_test['MAG_vista-H'] < 24]\n",
        "\n",
        "# Extract features for test set\n",
        "all_spectra_test = np.stack(df_test['combined_spectrum'].values)\n",
        "all_skyFlux_test = np.stack(df_test['combined_skyMask'].values)\n",
        "\n",
        "# Normalization of validation spectra\n",
        "all_spectra_test_normalized = all_spectra_test / max_value_train\n",
        "\n",
        "# rebin spectra with interpolation\n",
        "all_spectra_test_normalized = rebin_spectrum_with_interpolation(all_spectra_test_normalized, 1)\n",
        "all_skyFlux_test = rebin_spectrum_with_interpolation(all_skyFlux_test, 1)\n",
        "\n",
        "# make X test # mic\n",
        "X_test = np.stack((all_spectra_test_normalized, all_skyFlux_test), axis=-1)\n",
        "\n",
        "# Extract labels for test set\n",
        "all_redshift_test = df_test['z'].values\n",
        "all_stellar_masses_test = df_test['log_m'].values\n",
        "all_sfr_test = np.log10(df_test['sfr'].values)\n",
        "\n",
        "# define labels\n",
        "Y_test = np.column_stack((all_redshift_test, all_stellar_masses_test, all_sfr_test))\n",
        "\n",
        "# Normalize test labels\n",
        "Y_test_normalized = (Y_test - Y_train_mean) / Y_train_std\n",
        "\n",
        "# read the wavelength axis, needed below for checks & visualisations\n",
        "wavelength_axis = np.stack(df_test['combined_vacuumWave'].values)\n",
        "\n",
        "# mic\n",
        "wavelength_axis = rebin_spectrum_with_interpolation(wavelength_axis, 1)[0]"
      ],
      "metadata": {
        "id": "qFTJKVTZwlwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelling arena"
      ],
      "metadata": {
        "id": "9YZQFIwgh3H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Create model \"\"\"\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Concatenate, BatchNormalization, Attention\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(input_shape):\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(4, (1, 2),  strides=(1, 1), activation='elu')(inputs)\n",
        "    # x = BatchNormalization()(x)\n",
        "    x = Conv2D(4, (100, 1), strides=(1, 1), activation='elu')(x)\n",
        "\n",
        "    x = Conv2D(4, (10, 1), strides=(1, 1), activation='elu')(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # First dense hidden layer\n",
        "    x = Dense(128, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)  # Dropout layer\n",
        "\n",
        "    # Attentional layer\n",
        "    # attention_output = Attention()([x, x])\n",
        "\n",
        "    # Second dense hidden layer\n",
        "    x = Dense(64, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)  # Dropout layer\n",
        "\n",
        "    # Third dense hidden layer\n",
        "    x = Dense(64, activation='elu')(x)\n",
        "    # # x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)  # Dropout layer\n",
        "\n",
        "    # Task-specific layers\n",
        "    # Redshift prediction\n",
        "    redshift_output = Dense(1, activation='linear', name='redshift')(x)\n",
        "\n",
        "    # Stellar mass prediction\n",
        "    stellar_mass_output = Dense(1, activation='linear', name='stellar_mass')(x)\n",
        "\n",
        "    # Star formation rate prediction\n",
        "    sfr_output = Dense(1, activation='linear', name='sfr')(x)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(inputs=inputs, outputs=[redshift_output, stellar_mass_output, sfr_output])\n",
        "\n",
        "    return model\n",
        "\n",
        "# intanza il modello\n",
        "model = create_model(np.expand_dims(X_train[0], -1).shape) # np.expand_dims(X_train[0], -1).shape = (12217, 2, 1)\n",
        "\n",
        "# check model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "cWaBqP9iwnb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Class for metrics tracking. Updates metrics visualization at each epoch \"\"\"\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class MetricsPlotter(Callback):\n",
        "    def __init__(self, task_names):\n",
        "        # Initialize the lists to store the metrics\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "        self.task_metrics = {task: {'train_mae': [], 'val_mae': []} for task in task_names}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Append the losses\n",
        "        self.train_loss.append(logs.get('loss'))\n",
        "        self.val_loss.append(logs.get('val_loss'))\n",
        "\n",
        "        # Append the task-specific metrics\n",
        "        for task in self.task_metrics.keys():\n",
        "            self.task_metrics[task]['train_mae'].append(logs.get(f'{task}_mae'))\n",
        "            self.task_metrics[task]['val_mae'].append(logs.get(f'val_{task}_mae'))\n",
        "\n",
        "        # Plot the metrics\n",
        "        self.plot_metrics(epoch)\n",
        "\n",
        "    def plot_metrics(self, epoch):\n",
        "        # rimuove il plot di prima\n",
        "        plt.clf()\n",
        "\n",
        "        # Determine the number of rows and columns for subplots\n",
        "        num_tasks = len(self.task_metrics)\n",
        "        num_plots = num_tasks + 1  # +1 for the total loss plot\n",
        "        cols = 2  # number of columns I like\n",
        "        rows = (num_plots + cols - 1) // cols  # Calculate rows needed\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axs = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
        "\n",
        "        # Plot total training and validation loss\n",
        "        axs[0, 0].plot(range(epoch+1), self.train_loss, label='Training Loss')\n",
        "        axs[0, 0].plot(range(epoch+1), self.val_loss, label='Validation Loss')\n",
        "        axs[0, 0].set_title('Total Loss')\n",
        "        axs[0, 0].legend()\n",
        "\n",
        "        # Plot task-specific metrics\n",
        "        plot_index = 1  # Start from the second plot\n",
        "        for task, metrics in self.task_metrics.items():\n",
        "            ax = axs[plot_index // cols, plot_index % cols]\n",
        "            ax.plot(range(epoch+1), metrics['train_mae'], label=f'{task} Training MAE')\n",
        "            ax.plot(range(epoch+1), metrics['val_mae'], label=f'{task} Validation MAE')\n",
        "            ax.set_title(f'{task.capitalize()} MAE')\n",
        "            ax.legend()\n",
        "            plot_index += 1\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "1HUWwxFZwrzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Compile & run  model \"\"\"\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss={'redshift': 'mse',\n",
        "                    'stellar_mass': 'mse',\n",
        "                    'sfr': 'mse'},\n",
        "              loss_weights={'redshift': 2.0, 'stellar_mass': 1.0, 'sfr': 1.0},\n",
        "              metrics={'redshift': 'mae', 'stellar_mass': 'mae', 'sfr': 'mae'})\n",
        "\n",
        "# Early Stopping Callback\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_redshift_loss',  # Here I choose to monitor the redhsift validation loss for the early stopping\n",
        "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored metric\n",
        ")\n",
        "\n",
        "\n",
        "# Instantiate the callback with the task names\n",
        "task_names = ['redshift', 'stellar_mass', 'sfr']\n",
        "metrics_plotter = MetricsPlotter(task_names=task_names)\n",
        "\n",
        "\n",
        "# Train model with validation data, and add callback with visualisation\n",
        "history = model.fit(X_train, {'redshift': Y_train_normalized[:, 0], 'stellar_mass': Y_train_normalized[:, 1], 'sfr': Y_train_normalized[:, 2]},\n",
        "                    validation_data=(X_val, {'redshift': Y_val_normalized[:, 0], 'stellar_mass': Y_val_normalized[:, 1], 'sfr': Y_val_normalized[:, 2]}),\n",
        "                    shuffle=True,\n",
        "                    epochs=300,\n",
        "                    batch_size=1024,\n",
        "                    callbacks=[early_stopping, metrics_plotter])\n"
      ],
      "metadata": {
        "id": "hTT7cWvfwt0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Save Model \"\"\"\n",
        "\n",
        "model.save('current-best-convNet-model-MOONS')\n"
      ],
      "metadata": {
        "id": "7YxztEuBwveh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check history \"\"\"\n",
        "\n",
        "def plot_history(history, task, early_stopping_epoch=None):\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    if task == 'total':\n",
        "        # Plot total training & validation loss values\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Total Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        if early_stopping_epoch is not None: plt.axvline(x=early_stopping_epoch, color='gray', linestyle='--')\n",
        "\n",
        "    else:\n",
        "        # Plot training & validation loss values for specific task\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history[task+'_loss'])\n",
        "        plt.plot(history.history['val_'+task+'_loss'])\n",
        "        plt.title('Model Loss for ' + task.capitalize())\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        if early_stopping_epoch is not None: plt.axvline(x=early_stopping_epoch, color='gray', linestyle='--')\n",
        "\n",
        "        # Plot training & validation MAE values for specific task\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history[task+'_mae'])\n",
        "        plt.plot(history.history['val_'+task+'_mae'])\n",
        "        plt.title('Model MAE for ' + task.capitalize())\n",
        "        plt.ylabel('MAE')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        if early_stopping_epoch is not None: plt.axvline(x=early_stopping_epoch, color='gray', linestyle='--')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# #Plot total loss and tasks metrics\n",
        "plot_history(history, 'total', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))\n",
        "plot_history(history, 'redshift', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))\n",
        "plot_history(history, 'stellar_mass', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))\n",
        "plot_history(history, 'sfr', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))\n"
      ],
      "metadata": {
        "id": "DPv2FBofwwHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check global predictions on test-set \"\"\"\n",
        "\n",
        "test_metrics = model.evaluate(X_test, {'redshift': Y_test_normalized[:, 0], 'stellar_mass': Y_test_normalized[:, 1], 'sfr': Y_test_normalized[:, 2]})\n",
        "\n",
        "# compute prediction\n",
        "predictions = np.array(model.predict(X_test))\n",
        "\n",
        "# Reshape predictions to remove the extra dimension and match Y_test\n",
        "predictions_reshaped = predictions.squeeze()\n",
        "\n",
        "\n",
        "def inverse_transform(normalized_values, means, stds):\n",
        "    return normalized_values * stds + means\n",
        "\n",
        "# Applying the inverse transformation to predictions\n",
        "predictions_rescaled = np.array([inverse_transform(predictions[i], Y_train_mean[i], Y_train_std[i]) for i in range(predictions.shape[0])])\n",
        "\n",
        "def plot_predictions(predicted, actual, task_name):\n",
        "    plt.scatter(actual, predicted, alpha=0.1)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Predicted vs Actual Values for {task_name}')\n",
        "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=4)\n",
        "    plt.show()\n",
        "\n",
        "# Plotting predictions vs actual values for each task\n",
        "for i, task_name in enumerate(['Redshift', 'Stellar Mass', 'SFR']):\n",
        "    plot_predictions(predictions_rescaled[i], Y_test[:, i], task_name)\n",
        "\n",
        "#%%\n",
        "\"\"\" Check residual on specific tasks \"\"\"\n",
        "\n",
        "# redshfit\n",
        "residuals = predictions_rescaled[0,:].squeeze() -Y_test[:, 0]\n",
        "plt.hist(residuals, bins=100)\n",
        "\n",
        "np.std(residuals)"
      ],
      "metadata": {
        "id": "LJ3kPZXHwx9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Post-processing Analyses\n",
        "\n",
        "In the individual spectrum check, choose the ID as in the example usage after the definition of the function"
      ],
      "metadata": {
        "id": "g68priNIjQrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "\"\"\" check individual spectra \"\"\"\n",
        "\n",
        "def plot_spectrum_with_halpha_and_saliency(index, X_test, Y_test, model, Y_train_mean, Y_train_std, wavelength_axis):\n",
        "    # Predict the redshift for the selected object\n",
        "    object_spectrum = np.expand_dims(X_test[index], axis=0)\n",
        "    predicted_redshift = model.predict(object_spectrum)[0][0][0]\n",
        "\n",
        "    # Inverse transform the predicted redshift\n",
        "    predicted_redshift_rescaled = inverse_transform(predicted_redshift, Y_train_mean[0], Y_train_std[0])\n",
        "\n",
        "    # Actual redshift\n",
        "    actual_redshift = Y_test[index, 0]\n",
        "\n",
        "    # H-alpha line wavelength in Ångström (rest frame)\n",
        "    h_alpha_rest = 6562.8\n",
        "\n",
        "    # Calculate the observed positions of the H-alpha line\n",
        "    predicted_h_alpha_observed = h_alpha_rest * (1 + predicted_redshift_rescaled)\n",
        "    actual_h_alpha_observed = h_alpha_rest * (1 + actual_redshift)\n",
        "\n",
        "\n",
        "    input_sample_tensor = tf.convert_to_tensor(object_spectrum, dtype=tf.float32)\n",
        "    # Compute the gradient (saliency map)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(input_sample_tensor)\n",
        "        prediction = model(input_sample_tensor)\n",
        "\n",
        "    gradient = tape.gradient(prediction, input_sample_tensor)[0]\n",
        "    processed_grad = tf.abs(gradient)\n",
        "    processed_grad /= tf.math.reduce_max(processed_grad)\n",
        "    processed_grad = processed_grad.numpy()\n",
        "\n",
        "    # Plot the spectrum with saliency map overlay\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Add vertical lines for predicted and actual H-alpha line positions\n",
        "    plt.axvline(predicted_h_alpha_observed, color='green', linestyle='--', label='Predicted H-alpha')\n",
        "    plt.axvline(actual_h_alpha_observed, color='red', linestyle='--', label='Actual H-alpha')\n",
        "\n",
        "    # Add an inset with redshift information\n",
        "    textstr = f'Predicted Redshift: {predicted_redshift_rescaled:.2f}\\n Actual Redshift: {actual_redshift:.2f}'\n",
        "    plt.gcf().text(0.75, 0.15, textstr, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "    # Overlay the saliency map in red\n",
        "    plt.plot(wavelength_axis, processed_grad[:, 0], label='Saliency Map', color='red', alpha=1, lw=0.2)\n",
        "\n",
        "    # Plot the spectrum\n",
        "    plt.plot(wavelength_axis, X_test[index, :, 0]/X_test[index, :, 0].max(), label='Spectrum', lw=0.2, alpha=1)\n",
        "\n",
        "\n",
        "    plt.xlabel('Wavelength (Å)')\n",
        "    plt.ylabel('Intensity / Saliency')\n",
        "    plt.title(f'Spectrum with Predicted and Actual H-alpha Line and Saliency Map (Object {index})')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "# visualize\n",
        "index = 704\n",
        "# special_ID = 333011988000028\n",
        "# index = np.where(df_test['ID'] == special_ID)[0][0]\n",
        "plot_spectrum_with_halpha_and_saliency(index, X_test, Y_test, model, Y_train_mean, Y_train_std, wavelength_axis)"
      ],
      "metadata": {
        "id": "lwfKgNXfw2Xx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}