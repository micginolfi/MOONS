{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6/gsWw4Tv3Wf4HvxiiiyY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeJokwB-Wzs3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Fri Jan 19\n",
        "\n",
        "@author: mginolfi\n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import callbacks\n",
        "import tensorflow as tf\n",
        "import scipy.ndimage\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.interpolate import interp1d\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" load training set, make X & Y dataset and normalise\"\"\"\n",
        "\n",
        "df_train = pd.read_pickle('train_dataset.pickle')\n",
        "\n",
        "# Extract features for training set\n",
        "all_spectra_train = np.stack(df_train['combined_spectrum'].values)\n",
        "\n",
        "# Normalization of training spectra\n",
        "max_value_train = all_spectra_train.max()\n",
        "all_spectra_train_normalized = all_spectra_train / max_value_train\n",
        "\n",
        "# make X train # mic\n",
        "X_train = all_spectra_train_normalized\n",
        "\n",
        "# Extract labels for training set\n",
        "all_redshift_train = df_train['z'].values\n",
        "all_stellar_masses_train = df_train['log_m'].values\n",
        "all_sfr_train = np.log10(df_train['sfr'].values)\n",
        "\n",
        "# define labels: make Y\n",
        "Y_train = np.column_stack((all_redshift_train, all_stellar_masses_train, all_sfr_train))\n",
        "\n",
        "# Calculate mean and standard deviation for each label type in the training set\n",
        "Y_train_mean =  Y_train.mean(axis=0)\n",
        "Y_train_std = Y_train.std(axis=0)\n",
        "\n",
        "# Normalize training labels\n",
        "Y_train_normalized = (Y_train - Y_train_mean) / Y_train_std\n",
        "\n",
        "del df_train"
      ],
      "metadata": {
        "id": "c6Bk_S8cXD_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" load validation set, make X & Y dataset and normalise\"\"\"\n",
        "\n",
        "df_val = pd.read_pickle('validation_dataset.pickle')\n",
        "\n",
        "# Extract features for validation set\n",
        "all_spectra_val = np.stack(df_val['combined_spectrum'].values)\n",
        "\n",
        "# Normalization of validation spectra\n",
        "all_spectra_val_normalized = all_spectra_val / max_value_train\n",
        "\n",
        "# make X val # mic\n",
        "X_val = all_spectra_val_normalized\n",
        "\n",
        "# Extract labels for validation set\n",
        "all_redshift_val = df_val['z'].values\n",
        "all_stellar_masses_val = df_val['log_m'].values\n",
        "all_sfr_val = np.log10(df_val['sfr'].values)\n",
        "\n",
        "# define labels\n",
        "Y_val = np.column_stack((all_redshift_val, all_stellar_masses_val, all_sfr_val))\n",
        "\n",
        "# Normalize validation labels\n",
        "Y_val_normalized = (Y_val - Y_train_mean) / Y_train_std\n",
        "\n",
        "del df_val"
      ],
      "metadata": {
        "id": "xxSZbb5OXLML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_pickle('test_dataset.pickle')\n",
        "\n",
        "df_test.columns\n",
        "\n",
        "# Extract features for test set\n",
        "all_spectra_test = np.stack(df_test['combined_spectrum'].values)\n",
        "# all_skyFlux_test = np.stack(df_test['combined_skyMask'].values)\n",
        "\n",
        "# Normalization of validation spectra\n",
        "all_spectra_test_normalized = all_spectra_test / max_value_train\n",
        "\n",
        "# make X test # mic\n",
        "X_test = all_spectra_test_normalized\n",
        "\n",
        "# Extract labels for test set\n",
        "all_redshift_test = df_test['z'].values\n",
        "all_stellar_masses_test = df_test['log_m'].values\n",
        "all_sfr_test = np.log10(df_test['sfr'].values)\n",
        "\n",
        "# define labels\n",
        "Y_test = np.column_stack((all_redshift_test, all_stellar_masses_test, all_sfr_test))\n",
        "\n",
        "# Normalize test labels\n",
        "Y_test_normalized = (Y_test - Y_train_mean) / Y_train_std\n",
        "\n",
        "# read the wavelength axis, needed below for checks & visualisations\n",
        "wavelength_axis = np.stack(df_test['combined_vacuumWave'].values)\n",
        "wavelength_axis = wavelength_axis[0]"
      ],
      "metadata": {
        "id": "h_Y_r2lMXQgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"   Modelling              \"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_joint_model(input_shape, encoding_dim):\n",
        "\n",
        "    # Encoder\n",
        "    inputs = Input(shape=input_shape)\n",
        "    encoded = Dense(1024, activation='elu')(inputs)\n",
        "    encoded = Dropout(0.3)(encoded)\n",
        "    encoded = Dense(512, activation='elu')(encoded)\n",
        "    encoded = Dropout(0.3)(encoded)\n",
        "\n",
        "    # Latent space\n",
        "    encoded = Dense(encoding_dim, activation='elu')(encoded)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = Dense(512, activation='elu')(encoded)\n",
        "    decoded = Dropout(0.3)(decoded)\n",
        "    decoded = Dense(1024, activation='elu')(decoded)\n",
        "    decoded = Dropout(0.3)(decoded)\n",
        "    decoded = Dense(input_shape, activation='sigmoid', name='decoded_output')(decoded)\n",
        "\n",
        "    # Regression\n",
        "    regression = Dense(64, activation='elu')(encoded)\n",
        "    regression = Dropout(0.3)(regression)\n",
        "    # regression = Dense(32, activation='elu')(regression)\n",
        "    # regression = Dropout(0.3)(regression)\n",
        "\n",
        "    # Task-specific outputs\n",
        "    redshift_output = Dense(1, activation='linear', name='redshift')(regression)\n",
        "    stellar_mass_output = Dense(1, activation='linear', name='stellar_mass')(regression)\n",
        "    sfr_output = Dense(1, activation='linear', name='sfr')(regression)\n",
        "\n",
        "    # Combined model\n",
        "    combined_model = Model(inputs, [decoded, redshift_output, stellar_mass_output, sfr_output])\n",
        "\n",
        "    return combined_model\n",
        "\n",
        "# Define parameters\n",
        "input_shape = X_train.shape[1]\n",
        "encoding_dim = 1000\n",
        "task_names = ['redshift', 'stellar_mass', 'sfr']\n",
        "\n",
        "# Create the joint model\n",
        "joint_model = create_joint_model(input_shape, encoding_dim)\n",
        "\n",
        "joint_model.summary()"
      ],
      "metadata": {
        "id": "DTSj4UzqXXLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Class for metrics tracking \"\"\"\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class MetricsPlotter(Callback):\n",
        "    def __init__(self, task_names, include_autoencoder=False):\n",
        "        self.include_autoencoder = include_autoencoder\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "        self.task_metrics = {task: {'train_mae': [], 'val_mae': []} for task in task_names}\n",
        "        if self.include_autoencoder:\n",
        "            self.autoencoder_loss = {'train': [], 'val': []}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.train_loss.append(logs.get('loss'))\n",
        "        self.val_loss.append(logs.get('val_loss'))\n",
        "        if self.include_autoencoder:\n",
        "            self.autoencoder_loss['train'].append(logs.get('decoded_output_loss'))\n",
        "            self.autoencoder_loss['val'].append(logs.get('val_decoded_output_loss'))\n",
        "\n",
        "        for task in self.task_metrics.keys():\n",
        "            self.task_metrics[task]['train_mae'].append(logs.get(f'{task}_mae'))\n",
        "            self.task_metrics[task]['val_mae'].append(logs.get(f'val_{task}_mae'))\n",
        "\n",
        "        self.plot_metrics(epoch)\n",
        "\n",
        "    def plot_metrics(self, epoch):\n",
        "        plt.clf()\n",
        "        num_tasks = len(self.task_metrics) + (1 if self.include_autoencoder else 0)\n",
        "        num_plots = num_tasks + 1\n",
        "        cols = 2\n",
        "        rows = (num_plots + cols - 1) // cols\n",
        "        fig, axs = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
        "\n",
        "        axs[0, 0].plot(range(epoch + 1), self.train_loss, label='Training Loss')\n",
        "        axs[0, 0].plot(range(epoch + 1), self.val_loss, label='Validation Loss')\n",
        "        axs[0, 0].set_title('Total Loss')\n",
        "        axs[0, 0].legend()\n",
        "\n",
        "        plot_index = 1\n",
        "        if self.include_autoencoder:\n",
        "            ax = axs[plot_index // cols, plot_index % cols]\n",
        "            ax.plot(range(epoch + 1), self.autoencoder_loss['train'], label='Autoencoder Training Loss')\n",
        "            ax.plot(range(epoch + 1), self.autoencoder_loss['val'], label='Autoencoder Validation Loss')\n",
        "            ax.set_title('Autoencoder Loss')\n",
        "            ax.legend()\n",
        "            plot_index += 1\n",
        "\n",
        "        for task, metrics in self.task_metrics.items():\n",
        "            ax = axs[plot_index // cols, plot_index % cols]\n",
        "            ax.plot(range(epoch + 1), metrics['train_mae'], label=f'{task} Training MAE')\n",
        "            ax.plot(range(epoch + 1), metrics['val_mae'], label=f'{task} Validation MAE')\n",
        "            ax.set_title(f'{task.capitalize()} MAE')\n",
        "            ax.legend()\n",
        "            plot_index += 1\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "4plBEnamXbhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "\"\"\" Compile & run  joint model \"\"\"\n",
        "\n",
        "losses = {\n",
        "    'decoded_output': 'mse',  # Loss for the autoencoder part\n",
        "    'redshift': 'mse',\n",
        "    'stellar_mass': 'mse',\n",
        "    'sfr': 'mse'\n",
        "}\n",
        "\n",
        "loss_weights = {\n",
        "    'decoded_output': 1,  # Adjust this weight as needed\n",
        "    'redshift': 2,\n",
        "    'stellar_mass': 0.5,\n",
        "    'sfr': 0.5\n",
        "}\n",
        "\n",
        "joint_model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                    loss=losses,\n",
        "                    loss_weights=loss_weights,\n",
        "                    metrics={'redshift': 'mae', 'stellar_mass': 'mae', 'sfr': 'mae'})\n",
        "\n",
        "\n",
        "# Early Stopping Callback\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_redshift_loss',  # Monitor the validation loss\n",
        "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,           # To log when training is stopped\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity.\n",
        ")\n",
        "\n",
        "\n",
        "# Instantiate the callback with the task names and include the autoencoder\n",
        "metrics_plotter = MetricsPlotter(task_names=task_names, include_autoencoder=True)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = joint_model.fit(\n",
        "    X_train,\n",
        "    [X_train, Y_train_normalized[:, 0], Y_train_normalized[:, 1], Y_train_normalized[:, 2]],\n",
        "    validation_data=(X_val, [X_val, Y_val_normalized[:, 0], Y_val_normalized[:, 1], Y_val_normalized[:, 2]]),\n",
        "    shuffle=True,\n",
        "    epochs=300,\n",
        "    batch_size=1026,\n",
        "    callbacks=[early_stopping, metrics_plotter]\n",
        ")\n"
      ],
      "metadata": {
        "id": "WNo1p4KuXdu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check history \"\"\"\n",
        "\n",
        "def plot_history(history, task, early_stopping_epoch=None):\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    if task == 'total':\n",
        "        # Plot total training & validation loss values\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Total Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        if early_stopping_epoch is not None: plt.axvline(x=early_stopping_epoch, color='gray', linestyle='--')\n",
        "\n",
        "    else:\n",
        "        # Plot training & validation loss values for specific task\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history[task+'_loss'])\n",
        "        plt.plot(history.history['val_'+task+'_loss'])\n",
        "        plt.title('Model Loss for ' + task.capitalize())\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        if early_stopping_epoch is not None: plt.axvline(x=early_stopping_epoch, color='gray', linestyle='--')\n",
        "\n",
        "        # Plot training & validation MAE values for specific task\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history[task+'_mae'])\n",
        "        plt.plot(history.history['val_'+task+'_mae'])\n",
        "        plt.title('Model MAE for ' + task.capitalize())\n",
        "        plt.ylabel('MAE')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        if early_stopping_epoch is not None: plt.axvline(x=early_stopping_epoch, color='gray', linestyle='--')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# #Plot total loss and tasks metrics\n",
        "plot_history(history, 'total', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))\n",
        "plot_history(history, 'redshift', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))\n",
        "plot_history(history, 'stellar_mass', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))\n",
        "plot_history(history, 'sfr', early_stopping_epoch=np.argmin(history.history['val_redshift_loss']))"
      ],
      "metadata": {
        "id": "hV7RC5ZPXgpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check global predictions on test-set \"\"\"\n",
        "\n",
        "test_metrics = joint_model.evaluate(X_test, {'redshift': Y_test_normalized[:, 0], 'stellar_mass': Y_test_normalized[:, 1], 'sfr': Y_test_normalized[:, 2]})\n",
        "\n",
        "def inverse_transform(normalized_values, means, stds):\n",
        "    return normalized_values * stds + means\n",
        "\n",
        "def plot_predictions(predicted, actual, task_name):\n",
        "    plt.scatter(actual, predicted, alpha=0.1)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Predicted vs Actual Values for {task_name}')\n",
        "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=4)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Compute predictions\n",
        "predictions = joint_model.predict(X_test)\n",
        "\n",
        "# predictions[0] contains the reconstructed spectra, which we might not need here\n",
        "# predictions[1], predictions[2], and predictions[3] contain the redshift, stellar mass, and SFR predictions respectively\n",
        "redshift_pred = predictions[1]\n",
        "stellar_mass_pred = predictions[2]\n",
        "sfr_pred = predictions[3]\n",
        "\n",
        "# Applying the inverse transformation to predictions\n",
        "redshift_pred_rescaled = inverse_transform(redshift_pred.squeeze(), Y_train_mean[0], Y_train_std[0])\n",
        "stellar_mass_pred_rescaled = inverse_transform(stellar_mass_pred.squeeze(), Y_train_mean[1], Y_train_std[1])\n",
        "sfr_pred_rescaled = inverse_transform(sfr_pred.squeeze(), Y_train_mean[2], Y_train_std[2])\n",
        "\n",
        "# Plotting predictions vs actual values for each task\n",
        "for i, task_name in enumerate(['Redshift', 'Stellar Mass', 'SFR']):\n",
        "    actual = Y_test[:, i]\n",
        "    if task_name == 'Redshift':\n",
        "        predicted = redshift_pred_rescaled\n",
        "    elif task_name == 'Stellar Mass':\n",
        "        predicted = stellar_mass_pred_rescaled\n",
        "    else: # 'SFR'\n",
        "        predicted = sfr_pred_rescaled\n",
        "\n",
        "    plot_predictions(predicted, actual, task_name)\n"
      ],
      "metadata": {
        "id": "1Q_AMBffXk9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check residual on specific tasks \"\"\"\n",
        "\n",
        "# redshfit\n",
        "residuals = redshift_pred_rescaled -Y_test[:, 0]\n",
        "plt.hist(residuals, bins=100)\n",
        "\n",
        "np.std(residuals)"
      ],
      "metadata": {
        "id": "vgTU8Pm-Xppj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" check individual spectra \"\"\"\n",
        "\n",
        "def plot_spectrum_with_halpha_and_saliency(index, X_test, Y_test, model, Y_train_mean, Y_train_std, wavelength_axis):\n",
        "    # Predict the redshift for the selected object\n",
        "    object_spectrum = X_test[index]\n",
        "    predictions = model.predict(np.expand_dims(object_spectrum, axis=0))\n",
        "\n",
        "    # Assuming the redshift prediction is the second output\n",
        "    predicted_redshift = predictions[1]\n",
        "\n",
        "    # If the predicted_redshift is an array with a single value, extract that value\n",
        "    if predicted_redshift.size == 1:\n",
        "        predicted_redshift = predicted_redshift.item()\n",
        "\n",
        "    # Inverse transform the predicted redshift\n",
        "    predicted_redshift_rescaled = inverse_transform(predicted_redshift, Y_train_mean[0], Y_train_std[0])\n",
        "\n",
        "    # Actual redshift\n",
        "    actual_redshift = Y_test[index, 0]\n",
        "\n",
        "    # H-alpha line wavelength in Ångström (rest frame)\n",
        "    h_alpha_rest = 6562.8\n",
        "\n",
        "    # Calculate the observed positions of the H-alpha line\n",
        "    predicted_h_alpha_observed = h_alpha_rest * (1 + predicted_redshift_rescaled)\n",
        "    actual_h_alpha_observed = h_alpha_rest * (1 + actual_redshift)\n",
        "\n",
        "\n",
        "    input_sample_tensor = tf.convert_to_tensor(object_spectrum, dtype=tf.float32)\n",
        "    input_sample_tensor = tf.expand_dims(input_sample_tensor, axis=0)  # Ensure it's 2D\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(input_sample_tensor)\n",
        "        prediction = model(input_sample_tensor)\n",
        "\n",
        "    gradient = tape.gradient(prediction, input_sample_tensor)\n",
        "    processed_grad = tf.abs(gradient)\n",
        "    processed_grad /= tf.reduce_max(processed_grad)  # Normalize\n",
        "    processed_grad = processed_grad.numpy().flatten()  # Convert to 1D numpy array\n",
        "\n",
        "\n",
        "    # Plot the spectrum with saliency map overlay\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Add vertical lines for predicted and actual H-alpha line positions\n",
        "    plt.axvline(predicted_h_alpha_observed, color='green', linestyle='--', label='Predicted H-alpha')\n",
        "    plt.axvline(actual_h_alpha_observed, color='red', linestyle='--', label='Actual H-alpha')\n",
        "\n",
        "\n",
        "    predicted_redshift_scalar = predicted_redshift_rescaled.item()  # if predicted_redshift_rescaled is a numpy array with a single value\n",
        "    actual_redshift_scalar = actual_redshift.item()  # if actual_redshift is a numpy array with a single value\n",
        "\n",
        "    # Add an inset with redshift information\n",
        "    textstr = f'Predicted Redshift: {predicted_redshift_scalar:.2f}\\nActual Redshift: {actual_redshift_scalar:.2f}'\n",
        "    plt.gcf().text(0.75, 0.15, textstr, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "    # Overlay the saliency map in red\n",
        "    plt.plot(wavelength_axis, processed_grad, label='Saliency Map', color='red', alpha=1, lw=0.1)\n",
        "\n",
        "    # Plot the spectrum\n",
        "    plt.plot(wavelength_axis, X_test[index]/X_test[index].max(), label='Spectrum', lw=1, alpha=1)\n",
        "\n",
        "\n",
        "    plt.xlabel('Wavelength (Å)')\n",
        "    plt.ylabel('Intensity / Saliency')\n",
        "    plt.title(f'Spectrum with Predicted and Actual H-alpha Line and Saliency Map (Object {index})')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# visualize\n",
        "index = 11\n",
        "# special_ID = 333011988000028\n",
        "# index = np.where(df_test['ID'] == special_ID)[0][0]\n",
        "plot_spectrum_with_halpha_and_saliency(index, X_test, Y_test, joint_model, Y_train_mean, Y_train_std, wavelength_axis)"
      ],
      "metadata": {
        "id": "Y-6kWUXsXsII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}