{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1SQVGd9CMseUtXGFHXp3RR7OsRABnvRo3",
      "authorship_tag": "ABX9TyNf1FXF7lRUGYJcE0mMqkXY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a1DxnjrMe5Q"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan 16\n",
        "\n",
        "@author: michele ginolfi\n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import scipy.ndimage\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" read from pickle \"\"\"\n",
        "\n",
        "reconstructed_df = pd.read_pickle('final_combined_dataset.pickle')\n",
        "\n",
        "# show keys\n",
        "reconstructed_df.columns\n",
        "\n",
        "all_spectra = np.stack(reconstructed_df['combined_spectrum'].values)\n",
        "# all_skyMask = np.stack(reconstructed_df['combined_skyMask'].values)\n",
        "all_skyFlux = np.stack(reconstructed_df['combined_skyFlux'].values)\n",
        "\n",
        "all_ID = reconstructed_df['ID'].values\n",
        "all_exposure_times = reconstructed_df['exposure_time'].values\n",
        "\n",
        "all_redshift = reconstructed_df['z'].values\n",
        "all_stellar_masses = reconstructed_df['log_m'].values\n",
        "all_sfr = np.log10(reconstructed_df['sfr'].values)"
      ],
      "metadata": {
        "id": "4V_CBOs-Nbb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" normalize spectra \"\"\"\n",
        "\n",
        "\n",
        "# Applica la normalizzazione min-max\n",
        "# min_val = all_spectra.min(axis=1, keepdims=True)\n",
        "# max_val = all_spectra.max(axis=1, keepdims=True)\n",
        "# all_spectra_normalized = (all_spectra - min_val) / (max_val - min_val)\n",
        "\n",
        "# Applica la normalizzazione rispetto al massimo della matrice\n",
        "all_spectra_normalized = all_spectra / all_spectra.max()\n",
        "\n",
        "\n",
        "all_skyMask_normalized = all_skyFlux"
      ],
      "metadata": {
        "id": "2iryxkpbNhHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" make X & Y datasets \"\"\"\n",
        "\n",
        "X = np.stack((all_spectra_normalized, all_skyMask_normalized), axis=-1)\n",
        "\n",
        "Y = np.column_stack((all_redshift, all_stellar_masses, all_sfr))"
      ],
      "metadata": {
        "id": "tNBukLerNi-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del reconstructed_df\n",
        "del all_spectra\n",
        "del all_skyFlux"
      ],
      "metadata": {
        "id": "jb8r5FD3UYgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"split data\"\"\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First Split: Train (including validation) and Test\n",
        "X_temp, X_test, Y_temp, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)  # 15% test\n",
        "\n",
        "# Second Split: Train and Validation from X_temp and Y_temp\n",
        "# Note: 15% of the remaining 85% is 0.1765 (approximately 17.65%)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_temp, Y_temp, test_size=0.1765, random_state=42)  # About 15% of total"
      ],
      "metadata": {
        "id": "E28DtaZsNk9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Normalize labels \"\"\"\n",
        "\n",
        "# Calculate mean and standard deviation for each label type in the training set\n",
        "Y_train_mean = Y_train.mean(axis=0)\n",
        "Y_train_std = Y_train.std(axis=0)\n",
        "\n",
        "# Normalize each label type in the training set\n",
        "Y_train_normalized = (Y_train - Y_train_mean) / Y_train_std\n",
        "\n",
        "# Normalize each label type in the validation set using training set statistics\n",
        "Y_val_normalized = (Y_val - Y_train_mean) / Y_train_std\n",
        "\n",
        "# Normalize each label type in the test set using training set statistics\n",
        "Y_test_normalized = (Y_test - Y_train_mean) / Y_train_std"
      ],
      "metadata": {
        "id": "kDAc6o3zNnU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Create model \"\"\"\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Concatenate, BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(input_shape):\n",
        "\n",
        "    # print(input_shape)\n",
        "    # Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(16, (1, 2),  strides=(1, 1), activation='elu')(inputs)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after convolution\n",
        "    x = Conv2D(16, (100, 1), strides=(4, 1), activation='elu')(x)\n",
        "    x = Conv2D(16, (2, 1), strides=(4, 1), activation='elu')(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # First dense hidden layer\n",
        "    # x = Dense(32, activation='relu')(inputs)\n",
        "\n",
        "    # First dense hidden layer\n",
        "    x = Dense(128, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after dense layer\n",
        "    x = Dropout(0.3)(x)  # Dropout layer\n",
        "\n",
        "    # Second dense hidden layer\n",
        "    x = Dense(64, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after dense layer\n",
        "    x = Dropout(0.3)(x)  # Dropout layer\n",
        "\n",
        "    # Third dense hidden layer\n",
        "    x = Dense(32, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after dense layer\n",
        "    x = Dropout(0.3)(x)  # Dropout layer\n",
        "\n",
        "    # Task-specific layers\n",
        "    # Redshift prediction\n",
        "    redshift_output = Dense(1, activation='linear', name='redshift')(x)\n",
        "\n",
        "    # Stellar mass prediction\n",
        "    stellar_mass_output = Dense(1, activation='linear', name='stellar_mass')(x)\n",
        "\n",
        "    # Star formation rate prediction\n",
        "    sfr_output = Dense(1, activation='linear', name='sfr')(x)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(inputs=inputs, outputs=[redshift_output, stellar_mass_output, sfr_output])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_model(np.expand_dims(X_train[0], -1).shape) # np.expand_dims(X_train[0], -1).shape = (12217, 2, 1)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "m8spCBJ2NpfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Compile model \"\"\"\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss={'redshift': 'mse',\n",
        "                    'stellar_mass': 'mse',\n",
        "                    'sfr': 'mse'},\n",
        "              loss_weights={'redshift': 1.0, 'stellar_mass': 1.0, 'sfr': 1.0},\n",
        "              metrics={'redshift': 'mae', 'stellar_mass': 'mae', 'sfr': 'mae'})\n",
        "\n",
        "# Early Stopping Callback\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_redshift_loss',  # Monitor the validation loss\n",
        "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,           # To log when training is stopped\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity.\n",
        ")\n",
        "\n",
        "\n",
        "# Train model with validation data\n",
        "history = model.fit(X_train, {'redshift': Y_train_normalized[:, 0], 'stellar_mass': Y_train_normalized[:, 1], 'sfr': Y_train_normalized[:, 2]},\n",
        "                    validation_data=(X_val, {'redshift': Y_val_normalized[:, 0], 'stellar_mass': Y_val_normalized[:, 1], 'sfr': Y_val_normalized[:, 2]}),\n",
        "                    epochs=400,\n",
        "                    batch_size=1024,\n",
        "                    callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "MshbslifNsNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check history \"\"\"\n",
        "\n",
        "def plot_history(history, task):\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history[task+'_loss'])\n",
        "    plt.plot(history.history['val_'+task+'_loss'])\n",
        "    plt.title('Model loss for ' + task)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history[task+'_mae'])\n",
        "    plt.plot(history.history['val_'+task+'_mae'])\n",
        "    plt.title('Model MAE for ' + task)\n",
        "    plt.ylabel('MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# plot tasks metrics\n",
        "plot_history(history, 'redshift')\n",
        "plot_history(history, 'stellar_mass')\n",
        "plot_history(history, 'sfr')\n"
      ],
      "metadata": {
        "id": "IMqZYqaCNw5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check predictions on test-set \"\"\"\n",
        "\n",
        "test_metrics = model.evaluate(X_test, {'redshift': Y_test_normalized[:, 0], 'stellar_mass': Y_test_normalized[:, 1], 'sfr': Y_test_normalized[:, 2]})\n",
        "\n",
        "# compute prediction\n",
        "predictions = np.array(model.predict(X_test))\n",
        "\n",
        "# Reshape predictions to remove the extra dimension and match Y_test\n",
        "predictions_reshaped = predictions.squeeze()\n",
        "\n",
        "\n",
        "def inverse_transform(normalized_values, means, stds):\n",
        "    return normalized_values * stds + means\n",
        "\n",
        "# Applying the inverse transformation to predictions\n",
        "predictions_rescaled = np.array([inverse_transform(predictions[i], Y_train_mean[i], Y_train_std[i]) for i in range(predictions.shape[0])])\n",
        "\n",
        "def plot_predictions(predicted, actual, task_name):\n",
        "    plt.scatter(actual, predicted, alpha=0.1)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Predicted vs Actual Values for {task_name}')\n",
        "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=4)\n",
        "    plt.show()\n",
        "\n",
        "# Plotting predictions vs actual values for each task\n",
        "for i, task_name in enumerate(['Redshift', 'Stellar Mass', 'SFR']):\n",
        "    plot_predictions(predictions_rescaled[i], Y_test[:, i], task_name)"
      ],
      "metadata": {
        "id": "1XCVsPkQNzDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = predictions_rescaled[0,:].squeeze() -Y_test[:, 0]\n",
        "plt.hist(residuals, bins=100)\n",
        "\n",
        "np.std(residuals)"
      ],
      "metadata": {
        "id": "MPDO4EQKN0uh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}