{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1SQVGd9CMseUtXGFHXp3RR7OsRABnvRo3",
      "authorship_tag": "ABX9TyMOHFAwKF5a+kpQWMoT014p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micginolfi/MOONS/blob/main/MOONS_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6a1DxnjrMe5Q"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan 16\n",
        "\n",
        "@author: mginolfi\n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import scipy.ndimage\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" read from pickle \"\"\"\n",
        "\n",
        "reconstructed_df = pd.read_pickle('final_combined_dataset.pickle')\n",
        "\n",
        "# show keys\n",
        "reconstructed_df.columns\n",
        "\n",
        "all_spectra = np.stack(reconstructed_df['combined_spectrum'].values)\n",
        "# all_skyMask = np.stack(reconstructed_df['combined_skyMask'].values)\n",
        "all_skyFlux = np.stack(reconstructed_df['combined_skyFlux'].values)\n",
        "\n",
        "all_ID = reconstructed_df['ID'].values\n",
        "all_exposure_times = reconstructed_df['exposure_time'].values\n",
        "\n",
        "all_redshift = reconstructed_df['z'].values\n",
        "all_stellar_masses = reconstructed_df['log_m'].values\n",
        "all_sfr = np.log10(reconstructed_df['sfr'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "4V_CBOs-Nbb9",
        "outputId": "a7b5edea-12d9-4d01-86dc-0fc974eb0870"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'final_combined_dataset.pickle'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-eb9257147600>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" read from pickle \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreconstructed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final_combined_dataset.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# show keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m    189\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_combined_dataset.pickle'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del reconstructed_df\n"
      ],
      "metadata": {
        "id": "37yvGCeFNeO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" normalize spectra \"\"\"\n",
        "\n",
        "\n",
        "# Applica la normalizzazione min-max\n",
        "# min_val = all_spectra.min(axis=1, keepdims=True)\n",
        "# max_val = all_spectra.max(axis=1, keepdims=True)\n",
        "# all_spectra_normalized = (all_spectra - min_val) / (max_val - min_val)\n",
        "\n",
        "# Applica la normalizzazione rispetto al massimo della matrice\n",
        "all_spectra_normalized = all_spectra / all_spectra.max()\n",
        "\n",
        "\n",
        "all_skyMask_normalized = all_skyFlux"
      ],
      "metadata": {
        "id": "2iryxkpbNhHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" make X & Y datasets \"\"\"\n",
        "\n",
        "X = np.stack((all_spectra_normalized, all_skyMask_normalized), axis=-1)\n",
        "\n",
        "Y = np.column_stack((all_redshift, all_stellar_masses, all_sfr))"
      ],
      "metadata": {
        "id": "tNBukLerNi-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"split data\"\"\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First Split: Train (including validation) and Test\n",
        "X_temp, X_test, Y_temp, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)  # 15% test\n",
        "\n",
        "# Second Split: Train and Validation from X_temp and Y_temp\n",
        "# Note: 15% of the remaining 85% is 0.1765 (approximately 17.65%)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_temp, Y_temp, test_size=0.1765, random_state=42)  # About 15% of total"
      ],
      "metadata": {
        "id": "E28DtaZsNk9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Normalize labels \"\"\"\n",
        "\n",
        "# Calculate mean and standard deviation for each label type in the training set\n",
        "Y_train_mean = Y_train.mean(axis=0)\n",
        "Y_train_std = Y_train.std(axis=0)\n",
        "\n",
        "# Normalize each label type in the training set\n",
        "Y_train_normalized = (Y_train - Y_train_mean) / Y_train_std\n",
        "\n",
        "# Normalize each label type in the validation set using training set statistics\n",
        "Y_val_normalized = (Y_val - Y_train_mean) / Y_train_std\n",
        "\n",
        "# Normalize each label type in the test set using training set statistics\n",
        "Y_test_normalized = (Y_test - Y_train_mean) / Y_train_std"
      ],
      "metadata": {
        "id": "kDAc6o3zNnU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Create model \"\"\"\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Concatenate, BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(input_shape):\n",
        "\n",
        "    # print(input_shape)\n",
        "    # Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(1, (1, 2),  strides=(1, 1), activation='elu')(inputs)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after convolution\n",
        "    x = Conv2D(1, (100, 1), strides=(4, 1), activation='elu')(x)\n",
        "    x = Conv2D(1, (2, 1), strides=(4, 1), activation='elu')(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # First dense hidden layer\n",
        "    # x = Dense(32, activation='relu')(inputs)\n",
        "\n",
        "    # First dense hidden layer\n",
        "    x = Dense(128, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after dense layer\n",
        "    x = Dropout(0.3)(x)  # Dropout layer\n",
        "\n",
        "    # Second dense hidden layer\n",
        "    x = Dense(64, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after dense layer\n",
        "    x = Dropout(0.3)(x)  # Dropout layer\n",
        "\n",
        "    # Third dense hidden layer\n",
        "    x = Dense(32, activation='elu')(x)\n",
        "    # x = BatchNormalization()(x)  # Batch Normalization after dense layer\n",
        "    x = Dropout(0.3)(x)  # Dropout layer\n",
        "\n",
        "    # Task-specific layers\n",
        "    # Redshift prediction\n",
        "    redshift_output = Dense(1, activation='linear', name='redshift')(x)\n",
        "\n",
        "    # Stellar mass prediction\n",
        "    stellar_mass_output = Dense(1, activation='linear', name='stellar_mass')(x)\n",
        "\n",
        "    # Star formation rate prediction\n",
        "    sfr_output = Dense(1, activation='linear', name='sfr')(x)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(inputs=inputs, outputs=[redshift_output, stellar_mass_output, sfr_output])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_model(np.expand_dims(X_train[0], -1).shape) # np.expand_dims(X_train[0], -1).shape = (12217, 2, 1)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "m8spCBJ2NpfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Compile model \"\"\"\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss={'redshift': 'mse',\n",
        "                    'stellar_mass': 'mse',\n",
        "                    'sfr': 'mse'},\n",
        "              loss_weights={'redshift': 1.0, 'stellar_mass': 1.0, 'sfr': 1.0},\n",
        "              metrics={'redshift': 'mae', 'stellar_mass': 'mae', 'sfr': 'mae'})\n",
        "\n",
        "# Early Stopping Callback\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_redshift_loss',  # Monitor the validation loss\n",
        "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,           # To log when training is stopped\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity.\n",
        ")\n",
        "\n",
        "\n",
        "# Train model with validation data\n",
        "history = model.fit(X_train, {'redshift': Y_train_normalized[:, 0], 'stellar_mass': Y_train_normalized[:, 1], 'sfr': Y_train_normalized[:, 2]},\n",
        "                    validation_data=(X_val, {'redshift': Y_val_normalized[:, 0], 'stellar_mass': Y_val_normalized[:, 1], 'sfr': Y_val_normalized[:, 2]}),\n",
        "                    epochs=400,\n",
        "                    batch_size=1024,\n",
        "                    callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "MshbslifNsNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check history \"\"\"\n",
        "\n",
        "def plot_history(history, task):\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history[task+'_loss'])\n",
        "    plt.plot(history.history['val_'+task+'_loss'])\n",
        "    plt.title('Model loss for ' + task)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history[task+'_mae'])\n",
        "    plt.plot(history.history['val_'+task+'_mae'])\n",
        "    plt.title('Model MAE for ' + task)\n",
        "    plt.ylabel('MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# plot tasks metrics\n",
        "plot_history(history, 'redshift')\n",
        "plot_history(history, 'stellar_mass')\n",
        "plot_history(history, 'sfr')\n"
      ],
      "metadata": {
        "id": "IMqZYqaCNw5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Check predictions on test-set \"\"\"\n",
        "\n",
        "test_metrics = model.evaluate(X_test, {'redshift': Y_test_normalized[:, 0], 'stellar_mass': Y_test_normalized[:, 1], 'sfr': Y_test_normalized[:, 2]})\n",
        "\n",
        "# compute prediction\n",
        "predictions = np.array(model.predict(X_test))\n",
        "\n",
        "# Reshape predictions to remove the extra dimension and match Y_test\n",
        "predictions_reshaped = predictions.squeeze()\n",
        "\n",
        "\n",
        "def inverse_transform(normalized_values, means, stds):\n",
        "    return normalized_values * stds + means\n",
        "\n",
        "# Applying the inverse transformation to predictions\n",
        "predictions_rescaled = np.array([inverse_transform(predictions[i], Y_train_mean[i], Y_train_std[i]) for i in range(predictions.shape[0])])\n",
        "\n",
        "def plot_predictions(predicted, actual, task_name):\n",
        "    plt.scatter(actual, predicted, alpha=0.1)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Predicted vs Actual Values for {task_name}')\n",
        "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=4)\n",
        "    plt.show()\n",
        "\n",
        "# Plotting predictions vs actual values for each task\n",
        "for i, task_name in enumerate(['Redshift', 'Stellar Mass', 'SFR']):\n",
        "    plot_predictions(predictions_rescaled[i], Y_test[:, i], task_name)"
      ],
      "metadata": {
        "id": "1XCVsPkQNzDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = predictions_rescaled[0,:].squeeze() -Y_test[:, 0]\n",
        "plt.hist(residuals, bins=100)\n",
        "\n",
        "np.std(residuals)"
      ],
      "metadata": {
        "id": "MPDO4EQKN0uh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}